---
layout: post
title: cnn_basic
date: 2023-12-24
---

## 卷积神经网络工作原理

[toc]

### 1、什么是卷积神经网络（CNN）

#### 1.1、卷积神经网络概述

* 神经认知机：将一个视觉模式分解成许多子模式（特征），然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。
* 卷积神经网络是多层感知机（MLP）的变种，由生物学家休博尔和维瑟尔在早期关于猫视觉皮层的研究发展而来，视觉皮层的细胞存在一个复杂的构造，这些细胞对视觉输入空间的子区域非常敏感，称之为感受野。
* CNN 一方面减少了权值的数量使得网络易于优；另一方面降低了模型的复杂度，也就是减小了过拟合的风险
* 图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建的过程
* 网络能够自行抽取图像的特征包括颜色、纹理、形状及图像的拓扑结构
* 在处理二维图像的问题上，特别是识别位移、缩放及其他形式扭曲不变性的应用上具有良好的鲁棒性和运算效率
* 几个重要的模型：

|                      名称                       |                             特点                             |
| :---------------------------------------------: | :----------------------------------------------------------: |
|                     LeNet5                      |                          第一个 CNN                          |
|                     AlexNet                     | 引入了ReLU和dropout，引入数据增强、池化相互之间有覆盖，三个卷积一个最大池化+三个全连接层 |
|                     VGGNet                      | 采用`1*1`和`3*3`的卷积核以及`2*2`的最大池化使得层数变得更深。常用VGGNet-16和VGGNet19 |
|              Google Inception Net               | 在控制了计算量和参数量的同时，获得了比较好的分类性能，和上面相比有几个大的改进：1、去除了最后的全连接层，而是用一个全局的平均池化来取代它； 2、引入Inception Module，这是一个4个分支结合的结构。所有的分支都用到了11的卷积，这是因为`1*1`性价比很高，可以用很少的参数达到非线性和特征变换。3、Inception V2第二版将所有的`5*5`变成2个`3*3`，而且提出来著名的Batch Normalization；4、Inception V3第三版把较大的二维卷积拆成了两个较小的一维卷积，加速运算、减少过拟合，同时还更改了Inception Module的结构。 |
| 微软ResNet残差神经网络(Residual Neural Network) | 1、引入高速公路结构，可以让神经网络变得非常深；2、ResNet第二个版本将ReLU激活函数变成y=x的线性函数 |

#### 1.2、CNN 发展过程

* 1986年Rumelhart等人提出了人工神经网络的反向传播算法：神经网络中存在大量的参数，存在**容易发生过拟合、训练时间长**的缺点，但是对比Boosting、Logistic回归、SVM等基于统计学习理论的方法（也可以看做具有一层隐层节点或不含隐层节点的学习模型，被称为浅层模型）来说，具有较大的优越性。

* 浅层学习模型通常要由人工的方法来获得好的样本特性，在此基础上进行识别和预测，因此方法的有效性在很大程度上受到特征提取的制。因此浅层模型没有深层模型好

* 2006年，Hinton提出了深度学习：

    * 多隐层的人工神经网络具有优异的特征学习能力，学习到的数据更能反映数据的本质特征有利于可视化或分类
    * 深度神经网络在训练上的难度，可以通过逐层无监督训练有效克服，

* 深度学习成功的原因：

    * 深度学习使用大规模数据获得良好的训练资源
    * 计算机硬件的飞速发展：特别是GPU的出现，使得训练大规模上网络成为可能

* 深度神经网络的基本思想是通过构建多层网络，对目标进行多层表示，以期通过多层的高层次特征来表示数据的抽象语义信息，获得更好的特征鲁棒性。

* 卷积神经网络是一种带有卷积结构的深度神经网络，卷积结构可以减少深层网络占用的内存量，其三个关键的操作，**其一是局部感受野，其二是权值共享，其三是pooling层**，有效的减少了网络的参数个数，缓解了模型的过拟合问题

    * 网络结构：

        * 卷积神经网络是一种多层的监督学习神经网络，隐含层的卷积层和池采样层是实现卷积神经网络特征提取功能的核心模块
        * 网络模型通过采用梯度下降法最小化损失函数，对网络中的权重参数逐层反向调节，通过频繁的迭代训练提高网络的精度。
        * 卷积神经网络的低隐层是由卷积层和最大池采样层交替组成，高层是全连接层对应传统多层感知器的隐含层和逻辑回归分类器。
        * 第一个全连接层的输入是由卷积层和子采样层进行特征提取得到的特征图像。最后一层输出层是一个分类器，可以采用逻辑回归，Softmax回归甚至是支持向量机对输入图像进行分类。
        * 输入图像统计和滤波器进行卷积之后，提取该局部特征，一旦该局部特征被提取出来之后，它与其他特征的位置关系也随之确定下来了，每个神经元的输入和前一层的局部感受野相连，每个特征提取层都紧跟一个用来求局部平均与二次提取的计算层，也叫特征映射层，网络的每个计算层由多个特征映射平面组成，平面上所有的神经元的权重相等。
        * 通常将输入层到隐藏层的映射称为一个特征映射，也就是通过卷积层得到特征提取层，经过pooling之后得到特征映射层。

    * 局部感受野与权值共享：

        * 局部感受野：由于图像的空间联系是局部的，每个神经元不需要对全部的图像做感受，只需要感受局部特征即可，然后在更高层将这些感受得到的不同的局部神经元综合起来就可以得到全局的信息了，这样可以减少连接的数目。
        * 权值共享：不同神经元之间的参数共享可以减少需要求解的参数，使用多种滤波器去卷积图像就会得到多种特征映射。权值共享其实就是对图像用同样的卷积核进行卷积操作，也就意味着第一个隐藏层的所有神经元所能检测到处于图像不同位置的完全相同的特征。其主要的能力就能检测到不同位置的同一类型特征，也就是卷积网络能很好的适应图像的小范围的平移性，即有较好的平移不变性（比如将输入图像的猫的位置移动之后，同样能够检测到猫的图像

    * 卷积层、下采样层、全连接层：

        * 卷积层：因为通过卷积运算我们可以提取出图像的特征，通过卷积运算可以使得原始信号的某些特征增强，并且降低噪声。

            用一个可训练的滤波器 fx 去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征map了），然后加一个偏置 bx，得到卷积层 Cx

        * 下采样层：因为对图像进行下采样，可以减少数据处理量同时保留有用信息。采样可以混淆特征的具体位置，因为某个特征找出来之后，它的位置已经不重要了，我们只需要这个特征和其他特征的相对位置，可以应对形变和扭曲带来的同类物体的变化。每邻域四个像素求和变为一个像素，然后通过标量 Wx+1 加权，再增加偏置 bx+1，然后通过一个 sigmoid 激活函数，产生一个大概缩小四倍的特征映射图Sx+1。 

        * 全连接层：采用 softmax 全连接，得到的激活值即卷积神经网络提取到的图片特征。

    * 卷积神经网络相比一般神经网络在图像理解中的优点：

        - 网络结构能够较好的适应图像的结构
        - 同时进行特征提取和分类，使得特征提取有助于特征分类
        - 权值共享可以减少网络的训练参数，使得神经网络结构变得简单，适应性更强

#### 1.3、CNN 的特征

* 良好的容错能力、并行处理能力和自学习能力。通过结构重组和减少权值将特征抽取功能融合进多层感知器，省略识别前复杂的图像特征抽取过程
* 是一个前溃式神经网络，能从一个二维图像中提取其拓扑结构，采用反向传播算法来优化网络结构，求解网络中的未知参数
* 一类特别设计用来处理二维数据的多层神经网络
* CNN通过结合局部感知区域、共享权重、空间或者时间上的降采样来充分利用数据本身包含的局部性等特征，优化网络结构，并且保证一定程度上的位移和变形的不变性
* CNN是一种深度的监督学习下的机器学习模型，具有极强的适应性，善于挖掘数据局部特征，提取全局训练特征和分类，它的权值共享结构网络使之更类似于生物神经网络，在模式识别各个领域都取得了很好的成果

#### 1.4、CNN 求解

CNN在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。

卷积网络执行的是监督训练，所以其样本集是由形如：**（输入向量，理想输出向量）**的向量对构成的。所有这些向量对，都应该是来源于网络即将模拟系统的实际“运行”结构，它们可以是从实际运行系统中采集来。

1）参数初始化

在开始训练前，所有的权都应该用一些不同的随机数进行初始化。“小随机数”用来保证网络不会因权值过大而进入饱和状态，从而导致训练失败；“不同”用来保证网络可以正常地学习。实际上，如果用相同的数去初始化权矩阵，则网络无学习能力。

2）训练过程

* 选定训练组，从样本集中分别随机地寻求N个样本作为训练组；
* 将各权值、阈值，置成小的接近于0的随机值，并初始化精度控制参数和学习率；
* 从训练组中取一个输入模式加到网络，并给出它的目标输出向量；
* 计算出中间层输出向量，计算出网络的实际输出向量；
* 将输出向量中的元素与目标向量中的元素进行比较，计算出输出误差；对于中间层的隐单元也需要计算出误差；、
* 依次计算出各权值的调整量和阈值的调整量；
* 调整权值和调整阈值；
* 当经历M后，判断指标是否满足精度要求，如果不满足，则返回(3)，继续迭代；如果满足就进入下一步；
* 训练结束，将权值和阈值保存在文件中。这时可以认为各个权值已经达到稳定，分类器已经形成。再一次进行训练，直接从文件导出权值和阈值进行训练，不需要进行初始化

### 2、卷积神经网络的各种层

* 卷积神经网络主要由这几类层构成：输入层、卷积层，ReLU层、池化（Pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。在实际应用中往往将卷积层与ReLU层共同称之为卷积层，**所以卷积层经过卷积操作也是要经过激活函数的**。
* 卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数，即神经元的权值w和偏差b；而ReLU层和池化层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。

#### 2.1、卷积层

* 卷积层是构建卷积神经网络的**核心层**，它产生了网络中大部分的**计算量**。注意是计算量而不是参数量。

1）卷积层的作用：

* 滤波器的作用：卷积层的参数是由一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，**但是深度和输入数据一致**。网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。
* 神经元的一个输出：神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）
* 降低参数的数量：卷积具有“权值共享”这样的特性，可以降低参数数量，达到降低计算开销，防止由于参数过多而造成过拟合。

2）感受野：

* 在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。让每个神经元只与输入数据的一个局部区域连接。**该连接的空间大小叫做神经元的感受野**（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。**在深度方向上，这个连接的大小总是和输入量的深度相等**。
* 连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致
* 假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）
* 有一点需要注意，对应一个感受野有75个权重，这75个权重是通过学习进行更新的，所以很大程度上这些权值之间是不相等（也就对于同一个卷积核，它对于与它连接的输入的每一层的权重都是独特的，不是同样的权重重复输入层层数那么多次就可以的）
* 前面的每一个层对应一个传统意义上的卷积模板，每一层与自己卷积模板做完卷积之后，再将各个层的结果加起来，再加上偏置，注意是**一个偏置**，无论输入输入数据是多少层，一个卷积核就对应一个偏置

3）神经元的空间排列

* 3个超参数控制着**输出数据体**的尺寸：深度（depth），步长（stride）和零填充（zero-padding）
* 输出数据体的深度：它是一个超参数，和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西，即图像的某些特征
* 在滑动滤波器的时候，必须指定步长

4）权值共享

* 在卷积层中权值共享是用来控制参数的数量。假如在一个卷积核中，每一个感受野采用的都是不同的权重值（卷积核的值不同），那么这样的网络中参数数量将是十分巨大的。
* 如果一个特征在计算某个空间位置 (x1,y1) 的时候有用，那么它在计算另一个不同位置 (x2,y2) 的时候也有用。
* 将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]，其中在每个深度切片上的结果都使用同样的权重和偏差获得的
* 在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集
* **不同深度的神经元不会公用相同的权重**

5）其他形式的卷积操作

* 1X1 卷积：
    * 如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）
    * 将这种卷积的结果看作是全连接层的一种实现方式
* 扩张卷积：
    * 在很少的层数内更快地汇集输入图片的大尺度特征
    * 对卷积进行扩张，那么这个有效感受野就会迅速增长

#### 2.2、池化层

* 通常在连续的卷积层之间会周期性地插入一个池化层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。
* 在池化的过程中基本不会进行另补充；池化前后深度不变

#### 2.4、全连接层



